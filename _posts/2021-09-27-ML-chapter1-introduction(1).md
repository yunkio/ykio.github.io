---
date: 2021-09-27
title: "Chapter 1. Introduction (1)"
categories: 
  - 머신러닝과 패턴인식
tags: 
  - 머신러닝
  - 패턴인식
toc: true  
toc_sticky: true 
---
# 1.0 프롤로그
주어진 데이터에서 특정한 패턴을 찾아내는 것은 다양한 분야에서 중요한 영역입니다. **패턴 인식**은 컴퓨터 알고리즘을 통해 데이터의 규칙성을 자동적으로 찾아내고, 이 규칙성을 이용하여 데이터를 각각의 카테고리로 분류하는 등의 일을 하는 분야입니다.

## 기본 용어

![image](/assets/images/ml/Figure1.1.png){: width="400"}{: .align-center} 
Figure 1.1 MNIST 데이터
{: style="text-align: center; font-size:0.7em;"}

위 그림은 미국 우편번호에서 가져온 손으로 쓴 숫자를 분류하기 위한 데이터의 예 입니다. $N$개의 숫자들 $\{\mathbf{x}_1,...,\mathbf{x}_N\}$을 훈련 집합으로 활용하여 모델의 매개변수들을 조절해 표적 벡터 $\mathbf{t}$를 찾는 것을 목표로 합니다. 각 $\mathbf{x}$에 대한 표적 벡터 $\mathbf{t}$, 즉 정답은 하나가 됩니다.

머신 러닝 알고리즘의 결과물은 $\mathbf{y}(\mathbf{x})$로 표현이 가능합니다. 새로운 숫자의 이미지 $\mathbf{x}$를 입력으로 받아서 $\mathbf{t}$와 같은 방식으로 부호화된 벡터 $\mathbf{y}$를 출력하게 됩니다.

$\mathbf{y}(\mathbf{x})$는 **훈련 단계**에서 훈련 집합을 바탕으로 결정됩니다. 이후 이 모델은 **시험 집합**에서 새로운 숫자 이미지들을 분류하는데 활용됩니다. 훈련 단계에서 사용되지 않았던 예시들을 올바르게 분류하는 능력을 **일반화** 성능이라고 합니다.

대부분의 경우 입력 변수들을 **전처리**하여 새로운 변수 공간으로 전환합니다. 가령 각각의 숫자 이미지의 크기가 같아지도록 변환, 축소, 확대 등을 할 수 있습니다. 이를 통해 각 숫자 클래스 내의 가변성을 줄이게 됩니다. 이러한 전처리 과정은 **특징 추출**이라고 불리기도 합니다. 또한 계산 과정의 속도를 높이기 위해 유용한 정보를 가지고 있으면서 빠르게 계산이 가능한 특징들을 찾아내어 사용하기도 합니다. 이러한 특징들의 수는 이미지의 전체 픽셀의 수보다 적기 때문에 **차원 감소**라고 합니다.

## 학습의 종류
주어진 훈련 데이터가 입력 벡터와 그에 해당하는 표적 벡터로 이루어지는 문제를 **지도 학습**이라고 합니다. 앞에서 살펴본 숫자 인식 예시처럼 분리된 카테고리 중 하나에 할당하는 종류의 지도 학습 문제는 **분류** 문제라고 합니다. 그리고 기대되는 출력값이 연속된 값일 경우 **회귀** 문제라고 합니다.

훈련 데이터가 표적 벡터 없이 오직 입력 벡터 $\mathbf{x}$ 로만 주어지는 경우에는 **비지도 학습** 문제라고 부릅니다. 비지도 학습의 예로는 비슷한 예시들의 집단을 찾는 **집단화** 문제, 입력 공간에서의 데이터의 분포를 찾는 **밀도 추정** 문제, 높은 차원의 데이터를 2차원 또는 3차원으로 투영하는 **시각화** 등이 있습니다.

마지막으로 **강화 학습**이 있습니다. 주어진 상황에서 보상을 최대화하기 위한 행동을 찾는 문제를 푸는 방법입니다. 지도 학습과 달리 입력값과 최적의 출력값이 주어지지 않으며 시행착오를 통해서 이들을 찾아내게 됩니다. 예를 들면 신경망과 적절한 강화 학습 테크닉을 사용하여 백개먼이라는 게임을 잘해내는 알고리즘을 훈련시킬 수 있습니다. 문제는 보상이 오직 게임이 끝났을 때 승리라는 형태로만 주어진다는 점입니다. 따라서 보상은 최종 결과에 이끄는 모든 선택지에 대해서 잘 분배되어야 합니다. 이것을 **신뢰 할당** *credit assignment* 문제 라고 부릅니다. 또 일반적으로 강화 학습에는 **탐사**와 **이용**의 트레이드 오프가 존재합니다. 탐사 과정에서는 새로운 종류의 행동을 시도하며, 이용 과정에서는 높은 보상을 주는 것으로 알려진 행동을 시행합니다. 이 균형을 잘 맞추는 것이 강화 학습에 있어서 중요한 문제입니다.




# 1.1 예시 : 다항식 곡선 피팅

![image](/assets/images/ml/Figure1.2.png){: width="400"}{: .align-center} 
Figure 1.2 N=10인 훈련 데이터 집합의 도식
{: style="text-align: center; font-size:0.7em;"}

예시로 실숫값의 입력 변수인 $x$를 관찰한 후 이 값을 바탕으로 타겟 변수인 $t$를 예측하는 문제를 들어보겠습니다. 이 예시에서는 $\sin(2\pi x)$ 함수를 활용하여 데이터를 만들었으며 타깃 변수에는 노이즈가 포함되었으며, $N$개의 관찰값 $x$로 이루어진 훈련 집합 $\mathbf{x} \equiv (x_1, ..., x_N)^\text{T}$와 그에 해당하는 표적값 $\mathbf{t} \equiv (t_1, ..., t_N)^\text{T}$가 주어졌습니다. 

우리의 목표는 훈련 집합들을 사용해서 새로운 입력값 $\hat{x}$가 주어졌을 때 타깃변수 $\hat{t}$를 예측하는 것입니다. 따라서 기저에 있는 함수 $\sin(2\pi x)$를 찾아내는 것이 예측 과정에 암시적으로 포함됩니다. 

## 선형 모델

$$y(x, \mathbf{w}) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M = \sum^M_{j=0}w_jx^j$$

$M$은 다항식의 차수을 의미합니다. 다항식의 계수 ${w_0, ..., w_M}$을  함께 모아서 벡터 $\mathbf{w}$로 표현할 수 있습니다. 다항 함수 $y(x, \mathbf{w})$는 $x$에 대해서는 비선형이지만, 계수 $\mathbf{w}$에 대해서는 선형입니다. 이와 같이 알려지지 않은 변수에 대해 선형인 함수들은 **선형 모델**이라고 불립니다.

## 오차 함수
우리는 다항식을 훈련 집합 데이터에 적합시켜서 계수들의 값들을 구할 수 있습니다. 훈련 집합의 표적값들과 함숫값 $y(x, \mathbf{w})$와의 오차를 측정하는 **오차 함수**를 정의하고 이 함수의 값을 최소화하는 방식으로 이루어집니다. 가장 널리 쓰이는 오차 함수 중 하나는 다음과 같습니다.

$$E(\mathbf{w}) = \frac12\sum^N_{n=1}\{y(x_n,\mathbf{w})-t_n\}^2$$

위 식과 같이 각각의 데이터 포인트 $x_n$에 대해서 예측치 $y(x_n, \mathbf{w})$와 해당 표적값 $t_n$ 사이의 오차를 제곱하여 합산하여 오차의 합을 구하게 됩니다.

이 오차 함수를 선택한 이유에 대해서는 이 글의 후반부에 더 자세히 설명하겠습니다. 지금은 이 함수의 결과값은 양수이며, 오직 함수 $y(x,\mathbf{w})$가 정확히 데이터 포인트들을 지날 때만 값이 0이 된다는 사실을 알아두면 됩니다. 

![image](/assets/images/ml/Figure1.3.png){: width="400"}{: .align-center} 
Figure 1.3 오차 함수
{: style="text-align: center; font-size:0.7em;"}

$E(\mathbf{w})$를 최소화하는 $\mathbf{w}$를 선택함으로써 이 문제를 해결 가능합니다. 오차 함수가 이차 다항식의 형태를 지니고 있기 때문에 계수에 대해 미분하면 $\mathbf{w}$에 대해 선형인 식을 구할 수 있고, 유일한 최적의 값인 $\mathbf{w}^\ast$를 찾아낼 수 있습니다. 따라서 결과에 해당하는 다항식은 $y(x,\mathbf{w}^\ast)$의 형태를 가집니다.

## 모델 결정 *Model Selection*

<figure class="half">
  <a href="/assets/images/ml/Figure1.4a.png">
  <img src="/assets/images/ml/Figure1.4a.png"></a>

  <a href="/assets/images/ml/Figure1.4b.png">
  <img src="/assets/images/ml/Figure1.4b.png"></a>
</figure>
<figure class="half">
  <a href="/assets/images/ml/Figure1.4c.png">
  <img src="/assets/images/ml/Figure1.4c.png"></a>

  <a href="/assets/images/ml/Figure1.4d.png">
  <img src="/assets/images/ml/Figure1.4d.png"></a>
</figure>
Figure 1.4 다양한 차수 $M$에 따른 주어지는 곡선 피팅
{: style="text-align: center; font-size:0.7em;"}

다항식의 차수 $M$을 결정하는 문제가 여전히 남아 있습니다. 이 문제가 **모델 결정**이라 불리는 콘셉트의 예시입니다. 위 그림은 $M = 0, 1, 3, 9$인 네 가지 경우에 대해 다항식을 피팅하는 예시입니다. 네 가지 예시 중에서는 삼차$(M = 3)$ 다항식의 경우가 $\sin(2\pi x)$를 가장 잘 표현하는 것으로 보입니다. 차수를 많이 높일 경우$(M=9)$에는 훈련 집합에 완벽한 피팅이 가능하여 $E(\mathbf{w}^\ast) = 0$이지만 피팅된 곡선이 심하게 진동하여 $\sin(2\pi x)$를 잘 표현하고 있지는 못 합니다. 이것을 **과적합**이라고 부릅니다.

$M$에 따른 일반화의 성능이 어떻게 변화하는지 정량적으로 살펴보겠습니다. 이를 위해 앞과 같은 과정을 사용하되 랜덤한 노이즈값만 다르게 적용하여 100개의 새 데이터 포인트로 이루어진 시험 집합을 만들어 훈련 집합과 시험 집합 각각에 대해서 $E(\mathbf{w}^\ast)$를 계산할 것입니다. 이를 통해 각각의 차수 $M$에 대해서 잔차가 어떻게 변화하는지 확인할 수 있습니다. 이를 위해 **평균 제곱근 오차** *root mean square error, RMS error*를 사용하겠습니다. 평균 제곱근 오차의 식은 다음과 같습니다.

$$E_\text{RMS} = \sqrt{2E(\mathbf{w}^\ast)/N}$$

N으로 나눔으로써 데이터 사이즈가 다른 경우에도 비교할 수 있으며, 제곱근을 취해 E_\text{RMS}가 표적값 $t$와 같은 크기를 가지도록 했습니다.

![image](/assets/images/ml/Figure1.5.png){: width="400"}{: .align-center} 
Figure 1.5 RMS 그래프
{: style="text-align: center; font-size:0.7em;"}

$M$ 값이 작은 경우에는 시험 집합의 오차가 상대적으로 큽니다. 낮은 차수의 다항식은 융통성이 없어 피팅된 다항식이 함수 $\sin(2\pi x)$의 진동을 잘 반영하지 못 합니다. 반면 $3 \leqq M \leqq 8$ 인 경우 시험 집합의 오차가 작고 피팅된 다항식이 $\sin(2\pi x)$를 잘 표현합니다.

$M = 9$인 경우는 앞서 살펴본 것과 같이 과적합되고 있습니다. 함수 $y(x,\mathbf{w}^\ast)$가 심하게 진동하고 있으므로 시험 집합의 오차가 굉장히 큽니다.

