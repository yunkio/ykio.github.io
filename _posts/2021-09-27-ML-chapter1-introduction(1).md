---
date: 2021-09-27
title: "Chapter 1. Introduction (1)"
categories: 
  - 머신러닝과 패턴인식
tags: 
  - 머신러닝
  - 패턴인식
  - 확률
toc: true  
toc_sticky: true 
---
# 1.0 프롤로그
주어진 데이터에서 특정한 패턴을 찾아내는 것은 다양한 분야에서 중요한 영역입니다. **패턴 인식**은 컴퓨터 알고리즘을 통해 데이터의 규칙성을 자동적으로 찾아내고, 이 규칙성을 이용하여 데이터를 각각의 카테고리로 분류하는 등의 일을 하는 분야입니다.

## 기본 용어

![image](/assets/images/ml/Figure1.1.png){: width="400"}{: .align-center} 
Figure 1.1 MNIST 데이터
{: style="text-align: center; font-size:0.7em;"}

위 그림은 미국 우편번호에서 가져온 손으로 쓴 숫자를 분류하기 위한 데이터의 예 입니다. $N$개의 숫자들 $\{\mathbf{x}_1,...,\mathbf{x}_N\}$을 훈련 집합으로 활용하여 모델의 매개변수들을 조절해 표적 벡터 $\mathbf{t}$를 찾는 것을 목표로 합니다. 각 $\mathbf{x}$에 대한 표적 벡터 $\mathbf{t}$, 즉 정답은 하나가 됩니다.

머신 러닝 알고리즘의 결과물은 $\mathbf{y}(\mathbf{x})$로 표현이 가능합니다. 새로운 숫자의 이미지 $\mathbf{x}$를 입력으로 받아서 $\mathbf{t}$와 같은 방식으로 부호화된 벡터 $\mathbf{y}$를 출력하게 됩니다.

$\mathbf{y}(\mathbf{x})$는 **훈련 단계**에서 훈련 집합을 바탕으로 결정됩니다. 이후 이 모델은 **시험 집합**에서 새로운 숫자 이미지들을 분류하는데 활용됩니다. 훈련 단계에서 사용되지 않았던 예시들을 올바르게 분류하는 능력을 **일반화** 성능이라고 합니다.

대부분의 경우 입력 변수들을 **전처리**하여 새로운 변수 공간으로 전환합니다. 가령 각각의 숫자 이미지의 크기가 같아지도록 변환, 축소, 확대 등을 할 수 있습니다. 이를 통해 각 숫자 클래스 내의 가변성을 줄이게 됩니다. 이러한 전처리 과정은 **특징 추출**이라고 불리기도 합니다. 또한 계산 과정의 속도를 높이기 위해 유용한 정보를 가지고 있으면서 빠르게 계산이 가능한 특징들을 찾아내어 사용하기도 합니다. 이러한 특징들의 수는 이미지의 전체 픽셀의 수보다 적기 때문에 **차원 감소**라고 합니다.

## 학습의 종류
주어진 훈련 데이터가 입력 벡터와 그에 해당하는 표적 벡터로 이루어지는 문제를 **지도 학습**이라고 합니다. 앞에서 살펴본 숫자 인식 예시처럼 분리된 카테고리 중 하나에 할당하는 종류의 지도 학습 문제는 **분류** 문제라고 합니다. 그리고 기대되는 출력값이 연속된 값일 경우 **회귀** 문제라고 합니다.

훈련 데이터가 표적 벡터 없이 오직 입력 벡터 $\mathbf{x}$ 로만 주어지는 경우에는 **비지도 학습** 문제라고 부릅니다. 비지도 학습의 예로는 비슷한 예시들의 집단을 찾는 **집단화** 문제, 입력 공간에서의 데이터의 분포를 찾는 **밀도 추정** 문제, 높은 차원의 데이터를 2차원 또는 3차원으로 투영하는 **시각화** 등이 있습니다.

마지막으로 **강화 학습**이 있습니다. 주어진 상황에서 보상을 최대화하기 위한 행동을 찾는 문제를 푸는 방법입니다. 지도 학습과 달리 입력값과 최적의 출력값이 주어지지 않으며 시행착오를 통해서 이들을 찾아내게 됩니다. 예를 들면 신경망과 적절한 강화 학습 테크닉을 사용하여 백개먼이라는 게임을 잘해내는 알고리즘을 훈련시킬 수 있습니다. 문제는 보상이 오직 게임이 끝났을 때 승리라는 형태로만 주어진다는 점입니다. 따라서 보상은 최종 결과에 이끄는 모든 선택지에 대해서 잘 분배되어야 합니다. 이것을 **신뢰 할당** *credit assignment* 문제 라고 부릅니다. 또 일반적으로 강화 학습에는 **탐사**와 **이용**의 트레이드 오프가 존재합니다. 탐사 과정에서는 새로운 종류의 행동을 시도하며, 이용 과정에서는 높은 보상을 주는 것으로 알려진 행동을 시행합니다. 이 균형을 잘 맞추는 것이 강화 학습에 있어서 중요한 문제입니다.




# 1.1 예시 : 다항식 곡선 피팅

![image](/assets/images/ml/Figure1.2.png){: width="400"}{: .align-center} 
Figure 1.2 N=10인 훈련 데이터 집합의 도식
{: style="text-align: center; font-size:0.7em;"}

예시로 실숫값의 입력 변수인 $x$를 관찰한 후 이 값을 바탕으로 타겟 변수인 $t$를 예측하는 문제를 들어보겠습니다. 이 예시에서는 $\sin(2\pi x)$ 함수를 활용하여 데이터를 만들었으며 타깃 변수에는 노이즈가 포함되었으며, $N$개의 관찰값 $x$로 이루어진 훈련 집합 $\mathbf{x} \equiv (x_1, ..., x_N)^\text{T}$와 그에 해당하는 표적값 $\mathbf{t} \equiv (t_1, ..., t_N)^\text{T}$가 주어졌습니다. 

우리의 목표는 훈련 집합들을 사용해서 새로운 입력값 $\hat{x}$가 주어졌을 때 타깃변수 $\hat{t}$를 예측하는 것입니다. 따라서 기저에 있는 함수 $\sin(2\pi x)$를 찾아내는 것이 예측 과정에 암시적으로 포함됩니다. 

## 선형 모델

$$y(x, \mathbf{w}) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M = \sum^M_{j=0}w_jx^j$$

$M$은 다항식의 차수을 의미합니다. 다항식의 계수 ${w_0, ..., w_M}$을  함께 모아서 벡터 $\mathbf{w}$로 표현할 수 있습니다. 다항 함수 $y(x, \mathbf{w})$는 $x$에 대해서는 비선형이지만, 계수 $\mathbf{w}$에 대해서는 선형입니다. 이와 같이 알려지지 않은 변수에 대해 선형인 함수들은 **선형 모델**이라고 불립니다.

## 오차 함수
우리는 다항식을 훈련 집합 데이터에 적합시켜서 계수들의 값들을 구할 수 있습니다. 훈련 집합의 표적값들과 함숫값 $y(x, \mathbf{w})$와의 오차를 측정하는 **오차 함수**를 정의하고 이 함수의 값을 최소화하는 방식으로 이루어집니다. 가장 널리 쓰이는 오차 함수 중 하나는 다음과 같습니다.

$$E(\mathbf{w}) = \frac12\sum^N_{n=1}\{y(x_n,\mathbf{w})-t_n\}^2$$

위 식과 같이 각각의 데이터 포인트 $x_n$에 대해서 예측치 $y(x_n, \mathbf{w})$와 해당 표적값 $t_n$ 사이의 오차를 제곱하여 합산하여 오차의 합을 구하게 됩니다.

이 오차 함수를 선택한 이유에 대해서는 이 글의 후반부에 더 자세히 설명하겠습니다. 지금은 이 함수의 결과값은 양수이며, 오직 함수 $y(x,\mathbf{w})$가 정확히 데이터 포인트들을 지날 때만 값이 0이 된다는 사실을 알아두면 됩니다. 

![image](/assets/images/ml/Figure1.3.png){: width="400"}{: .align-center} 
Figure 1.3 오차 함수
{: style="text-align: center; font-size:0.7em;"}

$E(\mathbf{w})$를 최소화하는 $\mathbf{w}$를 선택함으로써 이 문제를 해결 가능합니다. 오차 함수가 이차 다항식의 형태를 지니고 있기 때문에 계수에 대해 미분하면 $\mathbf{w}$에 대해 선형인 식을 구할 수 있고, 유일한 최적의 값인 $\mathbf{w}^\ast$를 찾아낼 수 있습니다. 따라서 결과에 해당하는 다항식은 $y(x,\mathbf{w}^\ast)$의 형태를 가집니다.

## 모델 결정 *Model Selection*

<figure class="half">
  <a href="/assets/images/ml/Figure1.4a.png">
  <img src="/assets/images/ml/Figure1.4a.png"></a>

  <a href="/assets/images/ml/Figure1.4b.png">
  <img src="/assets/images/ml/Figure1.4b.png"></a>
</figure>
<figure class="half">
  <a href="/assets/images/ml/Figure1.4c.png">
  <img src="/assets/images/ml/Figure1.4c.png"></a>

  <a href="/assets/images/ml/Figure1.4d.png">
  <img src="/assets/images/ml/Figure1.4d.png"></a>
</figure>
Figure 1.4 다양한 차수 $M$에 따른 주어지는 곡선 피팅
{: style="text-align: center; font-size:0.7em;"}

다항식의 차수 $M$을 결정하는 문제가 여전히 남아 있습니다. 이 문제가 **모델 결정**이라 불리는 콘셉트의 예시입니다. 위 그림은 $M = 0, 1, 3, 9$인 네 가지 경우에 대해 다항식을 피팅한 결과입니다. 네 가지 예시 중에서는 삼차$(M = 3)$ 다항식의 경우가 $\sin(2\pi x)$를 가장 잘 표현하는 것으로 보입니다. 차수를 많이 높일 경우$(M=9)$에는 훈련 집합에 완벽한 피팅이 가능하여 $E(\mathbf{w}^\ast) = 0$이지만 피팅된 곡선이 심하게 진동하여 $\sin(2\pi x)$를 잘 표현하고 있지는 못 합니다. 이것을 **과적합**이라고 부릅니다.

$M$에 따른 일반화의 성능이 어떻게 변화하는지 정량적으로 살펴보겠습니다. 이를 위해 앞과 같은 과정을 사용하되 랜덤한 노이즈값만 다르게 적용하여 100개의 새 데이터 포인트로 이루어진 시험 집합을 만들어 훈련 집합과 시험 집합 각각에 대해서 $E(\mathbf{w}^\ast)$를 계산할 것입니다. 이를 통해 각각의 차수 $M$에 대해서 잔차가 어떻게 변화하는지 확인할 수 있습니다. 이를 위해 **평균 제곱근 오차** *root mean square error, RMS error*를 사용하겠습니다. 평균 제곱근 오차의 식은 다음과 같습니다.

$$E_\text{RMS} = \sqrt{2E(\mathbf{w}^\ast)/N}$$

N으로 나눔으로써 데이터 사이즈가 다른 경우에도 비교할 수 있으며, 제곱근을 취해 $E_\text{RMS}$가 표적값 $t$와 같은 크기를 가지도록 했습니다.

![image](/assets/images/ml/Figure1.5.png){: width="400"}{: .align-center} 
Figure 1.5 RMS 그래프
{: style="text-align: center; font-size:0.7em;"}

$M$ 값이 작은 경우에는 시험 집합의 오차가 상대적으로 큽니다. 낮은 차수의 다항식은 융통성이 없어 피팅된 다항식이 함수 $\sin(2\pi x)$의 진동을 잘 반영하지 못 합니다. 반면 $3 \leqq M \leqq 8$ 인 경우 시험 집합의 오차가 작고 피팅된 다항식이 $\sin(2\pi x)$를 잘 표현합니다. $M = 9$인 경우는 앞서 살펴본 것과 같이 과적합되고 있습니다. 함수 $y(x,\mathbf{w}^\ast)$가 심하게 진동하고 있으므로 시험 집합의 오차가 굉장히 큽니다.

차수 $M$에 따른 계수 $\mathbf{w}^\ast$를 살펴보면 $M$이 커짐에 따라 계숫값의 단위 역시 커집니다. $M=3$일 때는 십의 자리에서 계숫값을 가지지만, $M=9$일 경우에는 약 백만 단위의 자리까지의 양수값의 계수와 음숫값의 계수가 번갈아 나타납니다. 더 큰 $M$ 값을 가진 유연한 다항식이 표적값들에 포함된 랜덤한 노이즈들에 정확하게 피팅되어 이러한 결과가 나타나게 됩니다.

## 데이터의 크기

<figure class="half">
  <a href="/assets/images/ml/Figure1.6a.png">
  <img src="/assets/images/ml/Figure1.6a.png"></a>

  <a href="/assets/images/ml/Figure1.6b.png">
  <img src="/assets/images/ml/Figure1.6b.png"></a>
</figure> 
Figure 1.6 데이터 갯수에 따른 결과 그래프
{: style="text-align: center; font-size:0.7em;"}

사용되는 데이터 집합의 크기가 달라지는 경우 어떤 변화가 있는지 살펴보겠습니다. 왼쪽은 데이터의 갯수가 15개인 경우,  오른쪽은 데이터의 갯수가 100개인 경우입니다. 이와 같이 모델의 복잡도가 일정할 경우 데이터 집합의 수가 늘어날수록 과적합 문제가 완화됩니다. 이는 즉 데이터 집합의 수가 클수록 더 복잡한 모델을 활용할 수 있다는 의미도 됩니다. 여태까지의 예시에 사용했던 최소 제곱법은 **최대 가능도** *maximum likelihood*의 사례입니다. 과적합 문제는 최대 가능도 방법의 성질 중 하나입니다. 

## 정규화

비교적 복잡하고 유연한 모델을 제한적인 숫자의 데이터 집합을 활용하여 피팅하기 위해 자주 사용되는 기법 중 하나는 **정규화**입니다. 오차 함수에 계수의 크기가 커지는 것을 막기 위한 패널티항을 추가하는 것입니다. 가장 단순한 형태는 각각의 계수들을 제곱하여 합하는 것입니다. 식은 다음과 같습니다.

$$\tilde{E}(\mathbf{w}) = \frac12\sum^N_{n=1}\{y(x_n,\mathbf{w})-t_n\}^2+\frac\lambda2\Vert\mathbf{w}\Vert^2$$

여기서 $\Vert\mathbf{w}\Vert^2 \equiv \mathbf{w}^\text{T}\mathbf{w} = w^2_0 + w^2_1 + ... + w^2_M$이며, 계수 $\lambda$가 정규화항의 제곱합 오류항에 대한 상대적인 중요도를 결정합니다. 이때 $w_0$의 경우 포함시키면 타깃 변수의 원점을 무엇으로 선택하느냐에 대해 결과가 종속되기 때문에 제외하기도 하며, 별도의 정규화 계수와 함께 다른 항을 만들어 포함시키기도 합니다.

위 식의 오차 함수의 최솟값을 찾는 문제 역시 닫힌 형식이기 때문에 미분을 통해 유일해를 찾아낼 수 있습니다. 통계학에선 이를 **수축법** *shrinkage method*라고 합니다. 이차 형식(quadratic) 정규화는 **리지 회귀** *ridge regression*이라고 부릅니다. 뉴럴 네트워크에서는 **가중치 감쇠** *weight decay* 라고도 합니다.

<figure class="half">
  <a href="/assets/images/ml/Figure1.7a.png">
  <img src="/assets/images/ml/Figure1.7a.png"></a>

  <a href="/assets/images/ml/Figure1.7b.png">
  <img src="/assets/images/ml/Figure1.7b.png"></a>
</figure> 

Figure 1.7 $M=9$ 다항식을 Figure 1.2의 데이터 집합에 피팅한 그래프, 정규화된 오차 함수 사용
{: style="text-align: center; font-size:0.7em;"}

위 그림은 정규화된 오류 함수를 활용하여 $M=9$를 피팅한 결과입니다. $\ln \lambda = -18$의 경우 과적합이 많이 줄어들었으며, 그 결과 다항식이 $\sin(2\pi x)$에 훨씬 더 가까워졌습니다. 하지만 너무 큰 $\lambda$ 값을 사용하면 좋지 않은 피팅 결과를 얻게 됩니다. 

|  | $\ln\lambda = -\infty$ | $\ln\lambda = -18$ | $\ln\lambda = 0$ |
|--|:--:|:--:|:--:|
|$w_0^\ast$|0.35|0.35|0.13|
|$w_1^\ast$|232.37|4.74|-0.05|
|$w_2^\ast$|-5321.83|-0.77|-0.06|
|$w_3^\ast$|48568.31|-31.97|-0.05|
|$w_4^\ast$|-231639.30|-3.89|-0.03|
|$w_5^\ast$|640042.26|55.28|-0.02|
|$w_6^\ast$|-1061800.52|41.32|-0.01|
|$w_7^\ast$|1042400.18|-45.95|-0.00|
|$w_8^\ast$|-557682.99|-91.53|0.00|
|$w_9^\ast$|125201.43|72.68|0.01|

각각의 다항식의 계숫값들을 살펴보면 원래의 의도대로 계숫값의 크기가 많이 줄었음을 확인할 수 있습니다.

![image](/assets/images/ml/Figure1.8.png){: width="400"}{: .align-center} 
Figure 1.8 $M=9$ 다항식에 대한 $\ln\lambda$값에 따른 잔차 제곱 평균값의 그래프
{: style="text-align: center; font-size:0.7em;"}

위 그림은 훈련 집합과 시험 집합 각각에 대해서 평균 제곱근 오차를 서로 다른 $\ln\lambda$값에 대해 그린 것으로, 정규화항이 일반화 오류에 어떤 영향을 미치는지 알 수 있습니다. 모델의 복잡도를 조절해서 과적합 정도를 통제할 수 있습니다.

## 검증 집합 *validation set*

지금까지의 결과를 바탕으로 모델 복잡도를 잘 선택하는 방법 하나를 생각해 볼 수 있습니다. 데이터를 **훈련 집합**과 **검증 집합**으로 나누는 것입니다. 훈련 집합은 $\mathbf{w}$를 결정하는 데 활용하고, 검증 집합은 모델 복잡도($M$나 $\lambda$)를 최적화하는 데 활용하는 방식입니다. 이 방법은 훈련 집합 데이터를 낭비하게 된다는 단점이 있습니다.

# 1.2 확률론
패턴 인식 분야에서 중요한 컨셉 중 하나는 **불확실성** 입니다. 불확실성은 측정할 때의 노이즈 및 제한된 데이터 집합 수 등에서 발생합니다. 확률론을 통해 불확실성을 계량화하고 조작하기 위한 이론적인 토대를 갖출 수 있습니다.

![image](/assets/images/ml/Figure1.9.png){: width="400"}{: .align-center} 
Figure 1.9 확률론 예시
{: style="text-align: center; font-size:0.7em;"}

확률론의 기본적인 컨셉에 대해 이해하기 위해 예시를 하나 살펴보겠습니다. 위의 그림에서 랜덤하게 상자 하나를 골라 임의로 동그라미 하나를 꺼내고, 동그라미의 색을 확인한 후 꺼냈던 상자에다가 도로 집어 넣는다고 해보겠습니다. 이때 빨간색 상자를 고를 확률은 40%, 파란색 상자를 고를 확률은 60%이며 각각의 동그라미를 꺼낼 확률은 동일하다고 가정하겠습니다. 

이 예시에서 상자의 정체는 바로 확률 변수입니다. 앞으로 상자를 확률 변수 $B$라고 지칭하겠습니다. 이 확률 변수 $B$는 $r$(빨간색 상자)와 $b$(파란색 상자) 두 개의 값을 가질 수 있습니다. 과일 역시 확률 변수 $F$로 지칭할 것이며, 확률 변수 $F$는 $g$(초록색)과 $o$(오렌지색)를 값으로 가질 수 있습니다.

어떤 사건의 확률을 어떤 특정 사건이 일어나는 횟수를 전체 시도의 횟수로 나눈 것으로 정의하겠습니다. 정의에 따라 확률값은 $[0,1]$ 안에 있습니다. 이에 따라 $p(B = r) = 4/10$, $p(B=b)=6/10$ 이라고 적을 수 있습니다. 각각의 사건은 상호 배타적이며 각각의 사건들이 모든 가능한 결괏값을 포함할 경우 확률들의 합은 1입니다. 


## 합의 법칙과 곱의 법칙

이제 우리는 확률의 두 가지 기본 법칙인 **합의 법칙**과 **곱의 법칙**에 대해 살펴보겠습니다.

![image](/assets/images/ml/Figure1.10.png){: width="400"}{: .align-center} 
Figure 1.10 합의 법칙과 곱의 법칙을 설명하기 위한 그림
{: style="text-align: center; font-size:0.7em;"}

위의 예시에서는 $X$와 $Y$라는 두 가지 확률 변수가 있습니다. $X$는 $x_i(i = 1, ..., M)$ 중 아무 값이나 취할 수 있고, $Y$는 $y_j(j = 1, ..., L)$ 중 아무 값이나 취할 수 있다고 해보겠습니다. $X$와 $Y$ 각각에서 표본을 추출하는 시도를 $N$번 하며, $X=x_i$, $Y=y_i$인 시도의 개수를 $n_{ij}$라고 표현하겠습니다. 또한 $Y$의 값과 상관없이 $X=x_i$인 시도의 숫자를 $c_i$로, $Y=y_j$인 시도의 숫자를 $r_j$로 표현할 것입니다.

### 합의 법칙

$X$가 $x_i$, $Y$가 $y_j$일 확률을 $p(X=x_i,Y=y_j)$로 적고 이를 $X=x_i$, $Y=y_j$일 **결합 확률** 이라고 합니다. 이는 $i$, $j$ 칸에 있는 포인트의 숫자를 전체 포인트들의 숫자로 나눠서 구할 수 있습니다. 
$$p(X=x_i, Y=y_j) = \frac{n_{ij}}N$$

여기서 $\lim N \rightarrow \infty$ 를 가정했습니다. 비슷하게 $Y$값과 무관하게 $X$가 $x_i$값을 가질 확률을 $p(X = x_i)$로 적을 수 있습니다.

$$p(X = x_i) = \frac{c_i}N$$

앞서 살펴보았던 figure 1.10에서 $i$열에 있는 사례의 숫자는 해당 열의 각 칸에 있는 사례의 숫자 합입니다. 이는 $c_i = \sum_jn_{ij}$로 표현 가능합니다. 따라서 위 두 식으로 다음을 도출해 낼 수 있습니다.

$$p(X=x_i) = \sum^L_{j=1}p(X=x_i, Y=y_j)$$

이것이 바로 **합의 법칙**이며 $p(X=x_i)$는 **주변 확률** *marginal probability* 이라고 불립니다.

### 곱의 법칙

이제 $X = x_i$인 사례들만 고려해보겠습니다. 이 중에서 $Y=y_{ij}$인 사례들의 비율을 생각해 볼 수 있으며 이를 확률 $p(Y=y_j\mid X = x_i)$로 적을 수 있습니다. 이를 **조건부 확률** 이라고 부릅니다. 

$$p(Y=y_j|X=x_i) = \frac{n_{ij}}{c_i}$$

위와 같이 $i$ 행에 있는 전체 포인트의 수와 $i, j$칸에 있는 포인트 수의 비율을 통해 계산할 수 있으며, 이로부터  **곱의 법칙**을 도출해 낼 수 있습니다.

$$\begin{aligned} p(X=x_i,Y=y_i) &= \frac{n_{ij}}N = \frac{n_{ij}}{c_i} \cdot \frac{c_i}{N} \\ &= p(Y = y_j|X=x_i)p(X=x_i)\end{aligned}$$

위의 합의 법칙과 곱의 법칙을 더 간단한 표현법을 사용해서 표현할 수 있습니다.

$$p(X) = \sum_Yp(X, Y) \\ p(X, Y) = p(Y|X)p(X)$$
{: .notice}

## 베이즈 정리

위에서 구해낸 곱의 법칙을 활용하여 조건부 확률 간의 관계인 다음 식을 도출해낼 수 있습니다.

$$p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}$$

이 식이 머신 러닝과 패턴 인식 전반에 걸쳐 아주 중요한 역할을 차지하는 **베이즈 정리**입니다. 합의 법칙을 사용해서 베이지안 정리의 분모를 분자에 있는 항들로 표현할 수 있습니다.

$$p(X) = \sum_Yp(X|Y)p(Y)$$

베이지안 정리의 분모는 정규화 상수로 볼 수 있습니다. 왼쪽 항을 모든 $Y$값에 대하여 합했을 때 1이 되도록 하는 역할입니다.

<figure class="half">
  <a href="/assets/images/ml/Figure1.11a.png">
  <img src="/assets/images/ml/Figure1.11a.png"></a>

  <a href="/assets/images/ml/Figure1.11b.png">
  <img src="/assets/images/ml/Figure1.11b.png"></a>
</figure>
<figure class="half">
  <a href="/assets/images/ml/Figure1.11c.png">
  <img src="/assets/images/ml/Figure1.11c.png"></a>

  <a href="/assets/images/ml/Figure1.11d.png">
  <img src="/assets/images/ml/Figure1.11d.png"></a>
</figure>
Figure 1.11 두 변수의 결합 분포
{: style="text-align: center; font-size:0.7em;"}

이를 직관적으로 도식화한 그림이며, 두 변수의 결합 분포를 예로 들었습니다. 표본의 수는 $N=60$이며 표본들은 결합 분포에서 랜덤으로 추출했습니다. 분포에서 제한된 수의 데이터만 추출했을 때는 오른쪽 위의 그림과 같이 확률을 모델할 수 있습니다. 데이터로부터 원 분포를 모델링하는 것은 통계적 패턴 인식의 핵심 중 하나입니다.

### 예시

어떤 한 상자를 선택했는데 그것이 파란색 상자였다고 가정해보겠습니다. 그러면 그 상황에서 초록색 공을 고를 확률은 3/4이고 따라서 $p(F=a\mid B=b) = 3/4$ 입니다. 이와 같은 방법으로 경우의 수를 정리할 수 있습니다.

$$p(F=g|B=r) = 1/4 \\ p(F=o|B=r) = 3/4 \\ p(F=g|B=b) = 3/4 \\ p(F=o|B=b) = 1/4$$

이제 위의 식에서 확률의 합의 법칙과 곱의 법칙을 적용하여 초록색 공을 고를 전체 확률을 계산할 수 있습니다.

$$\begin{aligned} p(F=g) &= p(F=g|B=r)p(B=r) + p(F=g|B=b)p(B=b) \\ &= \frac14 \times \frac4{10} +  \frac34 \times \frac6{10} = \frac{11}{20} \end{aligned} $$

여기에 다시 합의 법칙을 적용하면 $p(F=o) = 1 - \frac{11}{20} = \frac{9}{20}$ 입니다.

이제 다른 예시를 들어보겠습니다. 어떤 공을 선택했는데 그 공이 오렌지색이고 이 오렌지색이 어떤 상자에서 나왔는지를 알고 싶다고 가정해 보겠습니다. 
이를 위해서는 공이 주어졌을 때 고른 상자가 어떤 것이었는지에 대한 조건부 확률을 계산해야 합니다. 베이지안 정리를 적용하여 조건부 확률을 뒤집으면 이 문제를 해결할 수 있습니다.

$$p(B=r|F=o) = \frac{p(F=o|B=r)p(B=r)}{p(F=o)} = \frac34\times\frac4{10}\times\frac{20}9 = \frac23$$

합의 법칙에 따라 $P(B=b \mid F=o) = 1 - \frac23 = \frac13$ 이 됩니다.

### 해석



